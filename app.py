import pandas as pd
from datetime import datetime
import uuid
import os
import streamlit as st

# ======================
# âš¡ User Count Tracking
# ======================
if "user_id" not in st.session_state:
    st.session_state.user_id = str(uuid.uuid4())
user_id = st.session_state.user_id

visits_file = "user_visits.csv"

if not os.path.exists(visits_file):
    df = pd.DataFrame(columns=["user_id", "visit_time"])
    df.to_csv(visits_file, index=False)

df = pd.read_csv(visits_file)

if "counted" not in st.session_state:
    if user_id not in df["user_id"].values:
        new_visit = pd.DataFrame([[user_id, datetime.now()]], columns=["user_id", "visit_time"])
        new_visit.to_csv(visits_file, mode="a", header=False, index=False)
    st.session_state.counted = True

st.sidebar.markdown(f"ğŸ‘¥ *Total Visitors100:* {df['user_id'].nunique()}")


# Step 1: Importing All Required Libraries for Multi-Search Agent RAG System

# Frontend
import streamlit as st

# Environment management
from dotenv import load_dotenv
import os
import time

# LangChain - LLM via Groq
from langchain_groq import ChatGroq

# LangChain - Document loaders for URLs, PDFs, TXT/MD
from langchain_community.document_loaders import (
    WebBaseLoader,
    PyPDFLoader,
    TextLoader
)

# LangChain - Text splitting
from langchain.text_splitter import RecursiveCharacterTextSplitter

# LangChain - Vector DB (Chroma)
from langchain_community.vectorstores import FAISS
# Split into chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)

# LangChain - Chains and prompts
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

# (Optional: Later when you build agent extensions)
# from langchain_community.tools import WikipediaQueryRun, ArxivQueryRun
# from langchain_community.utilities import WikipediaAPIWrapper, ArxivAPIWrapper


#step 2
from dotenv import load_dotenv
import os

load_dotenv()

from langchain_groq import ChatGroq

#llm = ChatGroq(api_key=os.getenv("GROQ_API_KEY"), model_name="mixtral-8x7b-32768")

llm = ChatGroq(api_key=os.getenv("GROQ_API_KEY"), model_name="llama3-70b-8192")

print(llm,"done")


from typing import List
from langchain.embeddings.base import Embeddings
from google.generativeai import embed_content
import google.generativeai as genai
import os
from dotenv import load_dotenv

# âœ… Load environment variables
load_dotenv()

# âœ… Configure Gemini API key from .env
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

class GeminiEmbedding(Embeddings):
    def __init__(self):
        self.model_name = "models/embedding-001"

    def _embed(self, text: str) -> List[float]:
        result = embed_content(
            model=self.model_name,
            content=text,
            task_type="retrieval_query",
            config={"output_dimensionality": 768}
        )
        return result["embedding"]

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return [self._embed(text) for text in texts]

    def embed_query(self, text: str) -> List[float]:
        return self._embed(text)



# ======================
# âš¡ Multi-Search Agent RAG System - Step 4 (Corrected)
# ======================

# Create Prompt Template
prompt = ChatPromptTemplate.from_template(
    """
    Answer the question based on the context below only.
    If the answer is not in the context, say "I don't know".

    <context>
    {context}
    </context>

    Question: {input}
    """
)

# Streamlit UI
st.title("ğŸ” Multi-Search Agent RAG System (Groq + LangChain)")

st.sidebar.header("ğŸ“¥ Ingest Your Data")

data_source = st.sidebar.radio("Select data sources:", ["URL", "PDF", "Text File", "CSV File"])

uploaded_file = None
input_url = None

if data_source == "URL":
    input_url = st.sidebar.text_input("Enter URL to ingest:")
elif data_source in ["PDF", "Text File"]:
    uploaded_file = st.sidebar.file_uploader(f"Upload your {data_source} file", type=["pdf", "txt", "md"])
elif data_source == "CSV File":
    uploaded_file = st.sidebar.file_uploader("Upload your CSV file", type=["csv"])

# Initialize session state holders
if "messages" not in st.session_state:
    st.session_state.messages = []
if "vector_store" not in st.session_state:
    st.session_state.vector_store = None
if "retrieval_chain" not in st.session_state:
    st.session_state.retrieval_chain = None


if st.sidebar.button("Ingest Data"):

    if data_source == "URL" and input_url:
        loader = WebBaseLoader(input_url)
    elif data_source == "PDF" and uploaded_file is not None:
        with open("temp_uploaded_file.pdf", "wb") as f:
            f.write(uploaded_file.read())
        loader = PyPDFLoader("temp_uploaded_file.pdf")
    elif data_source == "Text File" and uploaded_file is not None:
        with open("temp_uploaded_file.txt", "wb") as f:
            f.write(uploaded_file.read())
        loader = TextLoader("temp_uploaded_file.txt")
    elif data_source == "CSV File" and uploaded_file is not None:
        import pandas as pd
        df = pd.read_csv(uploaded_file)
        csv_text = df.to_string(index=False)  # convert DataFrame to plain text
        with open("temp_uploaded_file.csv.txt", "w", encoding="utf-8") as f:
            f.write(csv_text)
        from langchain_community.document_loaders import TextLoader
        loader = TextLoader("temp_uploaded_file.csv.txt")

    else:
        st.error("âš  Please provide a valid input for the selected data source.")
        st.stop()

    st.info("Loading and processing documents...")

    docs = loader.load()
    if not docs:
        st.error("âŒ Failed to load documents. Please check your file content.")
        st.stop()

    # Split into chunks
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    documents = text_splitter.split_documents(docs)

    from langchain_community.vectorstores import FAISS

    # Create vector store from documents and embeddings
    st.session_state.vector_store = FAISS.from_documents(documents, GeminiEmbeddings())

    # Optional: Save it for reuse
    st.session_state.vector_store.save_local("faiss_index")

    st.success("âœ… Data ingestion and vector store setup complete! You can now ask questions below.")

    # Create retrieval chain
    document_chain = create_stuff_documents_chain(llm, prompt)
    st.session_state.retrieval_chain = create_retrieval_chain(
        retriever=st.session_state.vector_store.as_retriever(),
        combine_docs_chain=document_chain
    )

st.sidebar.markdown("ğŸ”¹ *Built with ğŸ’“ by chantibabusambangi@gmail.com*")

# Only allow question input if retrieval_chain is ready
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
if (
    st.session_state.retrieval_chain is not None
    and st.session_state.vector_store is not None
    and len(st.session_state.vector_store.index_to_docstore_id) > 0
):
    user_query = st.chat_input("Ask your question:")

    
    if user_query:
        with st.spinner("Generating answer..."):
            start_time = time.time()
            response = st.session_state.retrieval_chain.invoke({"input": user_query})
            elapsed = time.time() - start_time

        st.subheader("Answer:")
        st.write(response.get('answer') or response.get('output') or response or "âš  No answer returned.")

        st.caption(f"âš¡ Response generated in {elapsed:.2f} seconds.")

        with st.expander("ğŸ” Full raw response (debug):"):
            st.json(response)

        if "context" in response and response["context"]:
            with st.expander("ğŸ“„ Show retrieved context chunks"):
                for doc in response["context"]:
                    st.write(doc.page_content)
                    st.write("---")
        else:
            st.info("âš  No retrieved context available for this query.")
else:
    st.warning("ğŸ‘ˆ Please ingest your data first using the sidebar before asking questions.")
